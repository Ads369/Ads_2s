{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "332aa0dc",
   "metadata": {
    "title": "Cell 1: Imports and Initialize"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import asyncio\n",
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "import mwclient\n",
    "import mwparserfromhell\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from aiogram import Bot, Dispatcher, Router\n",
    "from aiogram.filters import Command\n",
    "from aiogram.types import Message\n",
    "from openai import OpenAI\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d16f5f9",
   "metadata": {
    "title": "Cell 1: Constants"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is already set in the environment.\n",
      "OpenAI is Ready\n",
      "TELEGRAM_BOT_TOKEN is already set in the environment.\n",
      "Telegram bot is Ready\n"
     ]
    }
   ],
   "source": [
    "# Constanats\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "SAVE_PATH = \"./winter_olympics_2022.csv\"\n",
    "GLOBAL_DATA = None\n",
    "SYSTEM_MESSAGE = \"You answer questions about the 2022 Winter Olympics.\"\n",
    "TOPIC = \"2022 Winter Olympics\"\n",
    "CATEGORY_TITLE = f\"Category:{TOPIC}\"\n",
    "WIKI_SITE = \"en.wikipedia.org\"\n",
    "SECTIONS_TO_IGNORE = set(\n",
    "    [\n",
    "        \"See also\",\n",
    "        \"References\",\n",
    "        \"External links\",\n",
    "        \"Further reading\",\n",
    "        \"Footnotes\",\n",
    "        \"Bibliography\",\n",
    "        \"Sources\",\n",
    "        \"Citations\",\n",
    "        \"Literature\",\n",
    "        \"Footnotes\",\n",
    "        \"Notes and references\",\n",
    "        \"Photo gallery\",\n",
    "        \"Works cited\",\n",
    "        \"Photos\",\n",
    "        \"Gallery\",\n",
    "        \"Notes\",\n",
    "        \"References and sources\",\n",
    "        \"References and notes\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def setup_secret_key(key_name: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Set up the OpenAI API key, either from environment or user input.\n",
    "    \"\"\"\n",
    "    _key_name = os.environ.get(key_name)\n",
    "\n",
    "    if _key_name:\n",
    "        print(f\"{key_name} is already set in the environment.\")\n",
    "        return _key_name\n",
    "\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "\n",
    "        load_dotenv()\n",
    "        _key_name = os.environ.get(f\"{key_name}\")\n",
    "        if _key_name:\n",
    "            print(f\"{key_name} loaded from .env file.\")\n",
    "            return _key_name\n",
    "    except ImportError:\n",
    "        print(\"Warning: dotenv package not found. Unable to load from .env file.\")\n",
    "\n",
    "    try:\n",
    "        import getpass\n",
    "\n",
    "        _key_name = getpass.getpass(f\"Enter {key_name}: \")\n",
    "        os.environ[f\"{key_name}\"] = _key_name\n",
    "        print(f\"{key_name} set from user input.\")\n",
    "        return _key_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting API key: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "OPENAI_KEY = setup_secret_key(\"OPENAI_API_KEY\")\n",
    "OPENAI_CLIENT = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "if OPENAI_CLIENT is not None:\n",
    "    print(\"OpenAI is Ready\")\n",
    "\n",
    "TELEGRAM_BOT_TOKEN = setup_secret_key(\"TELEGRAM_BOT_TOKEN\")\n",
    "if TELEGRAM_BOT_TOKEN is None:\n",
    "    raise ValueError(\"Telegram bot token is not set.\")\n",
    "bot = Bot(TELEGRAM_BOT_TOKEN)\n",
    "if  bot is not None:\n",
    "    print(\"Telegram bot is Ready\")\n",
    "dp = Dispatcher()\n",
    "router = Router()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ea588e35",
   "metadata": {
    "title": "Cell 3: Fix All errors"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d136d1c",
   "metadata": {
    "title": "Cell 3: Utile functions"
   },
   "outputs": [],
   "source": [
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    # Функция возвращает число токенов в строке для заданной модели\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def clip_tokens(text: str, length: int = 4096, model: str = GPT_MODEL) -> str:\n",
    "    # Функция обрезает строку до заданного числа токенов, учитывая модель\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text=text)\n",
    "\n",
    "    token_value = [\n",
    "        encoding.decode_single_token_bytes(token).decode(\"utf-8\", errors=\"ignore\")\n",
    "        for token in tokens[0:length]\n",
    "    ]\n",
    "    return \"\".join(token_value)\n",
    "\n",
    "\n",
    "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str]:\n",
    "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter),\n",
    "    пытаясь сбалансировать токены с каждой стороны.\n",
    "    \"\"\"\n",
    "\n",
    "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
    "    chunks = string.split(delimiter)\n",
    "    if len(chunks) == 1:\n",
    "        return [string, \"\"]  # разделитель не найден\n",
    "    elif len(chunks) == 2:\n",
    "        return chunks  # нет необходимости искать промежуточную точку\n",
    "    else:\n",
    "        # Считаем токены\n",
    "        total_tokens = num_tokens(string)\n",
    "        halfway = total_tokens // 2\n",
    "        # Предварительное разделение по середине числа токенов\n",
    "        best_diff = halfway\n",
    "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            left = delimiter.join(chunks[: i + 1])\n",
    "            left_tokens = num_tokens(left)\n",
    "            diff = abs(halfway - left_tokens)\n",
    "            if diff >= best_diff:\n",
    "                break\n",
    "            else:\n",
    "                best_diff = diff\n",
    "\n",
    "        # TODO Fix errors\n",
    "        left = delimiter.join(chunks[:i])\n",
    "        right = delimiter.join(chunks[i:])\n",
    "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
    "        return [left, right]\n",
    "\n",
    "\n",
    "def truncated_string(\n",
    "    string: str,\n",
    "    model: str,\n",
    "    max_tokens: int,\n",
    "    print_warning: bool = True,\n",
    ") -> str:\n",
    "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\n",
    "\n",
    "    Args:\n",
    "        string (str): Строка.\n",
    "        model (str): Модель токенизации.\n",
    "        max_tokens (int): Максимальное число разрешенных токенов.\n",
    "        print_warning (bool, optional): Флаг вывода предупреждения. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: Обрезанная строка.\n",
    "    \"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    encoded_string = encoding.encode(string)\n",
    "    # Обрезаем строку и декодируем обратно\n",
    "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
    "    if print_warning and len(encoded_string) > max_tokens:\n",
    "        print(\n",
    "            f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\"\n",
    "        )\n",
    "    # Усеченная строка\n",
    "    return truncated_string\n",
    "\n",
    "\n",
    "def split_strings_from_subsection(\n",
    "    subsection: tuple[list[str], str],\n",
    "    max_tokens: int = 1000,\n",
    "    model: str = GPT_MODEL,\n",
    "    max_recursion: int = 5,\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
    "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
    "\n",
    "    Args:\n",
    "        subsection (tuple[list[str], str]): Кортеж с родительскими заголовками и текстом.\n",
    "        model (str, optional): Модель токенизации. Defaults to GPT_MODEL.\n",
    "        max_recursion (int, optional): Максимальное число рекурсий. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Список строк, разделенных на части секции.\n",
    "    \"\"\"\n",
    "    titles, text = subsection\n",
    "    string = \"\\n\\n\".join(titles + [text])\n",
    "    num_tokens_in_string = num_tokens(string)\n",
    "    # Если длина соответствует допустимой, то вернет строку\n",
    "    if num_tokens_in_string <= max_tokens:\n",
    "        return [string]\n",
    "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
    "    elif max_recursion == 0:\n",
    "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
    "    # иначе разделим пополам и выполним рекурсию\n",
    "    else:\n",
    "        titles, text = subsection\n",
    "        for delimiter in [\n",
    "            \"\\n\\n\",\n",
    "            \"\\n\",\n",
    "            \". \",\n",
    "        ]:  # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
    "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
    "            if left == \"\" or right == \"\":\n",
    "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
    "                continue\n",
    "            else:\n",
    "                # применим рекурсию на каждой половине\n",
    "                results = []\n",
    "                for half in [left, right]:\n",
    "                    half_subsection = (titles, half)\n",
    "                    half_strings = split_strings_from_subsection(\n",
    "                        half_subsection,\n",
    "                        max_tokens=max_tokens,\n",
    "                        model=model,\n",
    "                        max_recursion=max_recursion\n",
    "                        - 1,  # уменьшаем максимальное число рекурсий\n",
    "                    )\n",
    "                    results.extend(half_strings)\n",
    "                return results\n",
    "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
    "    return [truncated_string(string, model=model, max_tokens=max_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c4b7bfd",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Markdown cells:"
   },
   "outputs": [],
   "source": [
    "# Алгоритм обучения Search-Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc4ea2fd",
   "metadata": {
    "title": "Cell 5: Data functions"
   },
   "outputs": [],
   "source": [
    "def get_preset_data():\n",
    "    embeddings_path = (\n",
    "        \"https://storage.yandexcloud.net/academy.ai/winter_olympics_2022.csv\"\n",
    "    )\n",
    "    df = pd.read_csv(embeddings_path)\n",
    "    df[\"embedding\"] = df[\"embedding\"].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "\n",
    "GLOBAL_DATA = get_preset_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2f8403b3",
   "metadata": {
    "title": "Cell 6"
   },
   "outputs": [],
   "source": [
    "# Функция поиска\n",
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Возвращает отранжированные строки и их релевантность запросу\n",
    "\n",
    "    Args:\n",
    "        query (str): пользовательский запрос\n",
    "        df (pd.DataFrame): DataFrame со столбцами text и embedding (база знаний)\n",
    "        relatedness_fn (callable, optional): функция схожести, по умолчанию косинусное расстояние\n",
    "        top_n (int, optional): выбор лучших n-результатов. По умолчанию 100.\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[str], list[float]]: кортеж из двух списков - строки и их релевантность\n",
    "    \"\"\"\n",
    "    query_embedding_response = OPENAI_CLIENT.embeddings.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "\n",
    "    # Получен токенизированный пользовательский запрос\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # Сравниваем пользовательский запрос с каждой токенизированной строкой DataFrame\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Сортируем по убыванию схожести полученный список\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Преобразовываем наш список в кортеж из списков\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "\n",
    "    # Возвращаем n лучших результатов\n",
    "    return strings[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff6c162d",
   "metadata": {
    "title": "Cell"
   },
   "outputs": [],
   "source": [
    "# Функция формирования запроса к chatGPT по пользовательскому вопросу и базе знаний\n",
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int,\n",
    ") -> str:\n",
    "    \"\"\"Возвращает сообщение для GPT с соответствующими исходными текстами,\n",
    "    извлеченными из фрейма данных (базы знаний).\n",
    "\n",
    "    Args:\n",
    "        query (str): пользовательский запрос\n",
    "        df (pd.DataFrame): DataFrame со столбцами text и embedding (база знаний)\n",
    "        model (str): модель\n",
    "        token_budget (int): ограничение на число отсылаемых токенов в модель\n",
    "\n",
    "    Returns:\n",
    "        str: сообщение для GPT с соответствующими исходными текстами,\n",
    "        извлеченными из\n",
    "    \"\"\"\n",
    "\n",
    "    # ранжирования базы знаний по пользовательскому запросу\n",
    "    strings, relatednesses = strings_ranked_by_relatedness(query, df)\n",
    "\n",
    "    # Шаблон инструкции для chatGPT\n",
    "    message = 'Use the below articles on the 2022 Winter Olympics to answer the subsequent question. If the answer cannot be found in the articles, write \"I could not find an answer.\"'\n",
    "\n",
    "    # Шаблон для вопроса\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "    # Добавляем к сообщению для chatGPT релевантные строки из базы знаний, пока не выйдем за допустимое число токенов\n",
    "    for string in strings:\n",
    "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
    "        if num_tokens(message + next_article + question, model=model) > token_budget:\n",
    "            break\n",
    "        else:\n",
    "            message += next_article\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cae4c8b9",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell"
   },
   "outputs": [],
   "source": [
    "def ask(\n",
    "    query: str,\n",
    "    df: pd.DataFrame | None = None,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096 - 500,\n",
    "    print_message: bool = False,\n",
    ") -> str | None:\n",
    "    \"\"\"Отвечает на вопрос, используя GPT и базу знаний.\n",
    "\n",
    "    Args:\n",
    "        query (str): пользовательский запрос\n",
    "        df (pd.DataFrame, optional): DataFrame со столбцами text и embedding (база знаний). Defaults to df.\n",
    "        model (str, optional): модель.\n",
    "        token_budget (int, optional): ограничение на число отсылаемых токенов в модель.\n",
    "        print_message (bool, optional): нужно ли выводить сообщение перед отправкой.\n",
    "\n",
    "    Returns:\n",
    "        str: ответ на вопрос\n",
    "    \"\"\"\n",
    "\n",
    "    if df is None:\n",
    "        if GLOBAL_DATA is None:\n",
    "            df = get_preset_data()\n",
    "        else:\n",
    "            df = GLOBAL_DATA\n",
    "\n",
    "    if df is None:\n",
    "        raise ValueError(\"df is not None\")\n",
    "\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "\n",
    "    if print_message:\n",
    "        print(message)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_MESSAGE,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    response = OPENAI_CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,  # type: ignore\n",
    "        temperature=0,  # гиперпараметр степени случайности при генерации текста.\n",
    "        # Влияет на то, как модель выбирает следующее слово в последовательности.\n",
    "    )\n",
    "    response_message = response.choices[0].message.content\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "194863b4",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell: Test  ask()"
   },
   "outputs": [],
   "source": [
    "# ask('How many records were set at the 2022 Winter Olympics?')\n",
    "# ask(\"Did Jamaica or Cuba have more athletes at the 2022 Winter Olympics?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "946d13df",
   "metadata": {
    "title": "Cell: search info"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создано 26 заголовков статей в категории Category:2022 Winter Olympics.\n"
     ]
    }
   ],
   "source": [
    "# Соберем заголовки всех статей\n",
    "def titles_from_category(\n",
    "    category: mwclient.listing.Category,\n",
    "    max_depth: int,  # type: ignore\n",
    ") -> set[str]:\n",
    "    \"\"\"Возвращает набор заголовков страниц в данной категории Википедии и ее подкатегориях.\n",
    "\n",
    "    Args:\n",
    "        category (mwclient.listing.Category): Категория статей Википедии.\n",
    "        max_depth (int): Глубина вложения статей.\n",
    "\n",
    "    Returns:\n",
    "        set[str]: Набор заголовков страниц в данной категории Википедии и ее подкатегориях.\n",
    "    \"\"\"\n",
    "    titles = set()\n",
    "    for cm in category.members():  # Перебираем вложенные объекты категории\n",
    "        if isinstance(cm, mwclient.page.Page):  # type: ignore\n",
    "            titles.add(cm.name)\n",
    "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0:  # type: ignore\n",
    "            # Если объект является категорией и глубина вложения не достигла максимальной\n",
    "            # вызываем рекурсивно функцию для подкатегории\n",
    "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1)\n",
    "            titles.update(deeper_titles)\n",
    "    return titles\n",
    "\n",
    "\n",
    "# Инициализация объекта MediaWiki\n",
    "site = mwclient.Site(WIKI_SITE)\n",
    "category_page = site.pages[CATEGORY_TITLE]\n",
    "# Получение множества всех заголовков категории с вложенностью на один уровень\n",
    "titles = titles_from_category(category_page, max_depth=1)\n",
    "print(f\"Создано {len(titles)} заголовков статей в категории {CATEGORY_TITLE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5cef113d",
   "metadata": {
    "title": "Cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено 264 секций на 26 страницах\n"
     ]
    }
   ],
   "source": [
    "# Функция возвращает список всех вложенных секций для заданной секции страницы Википедии\n",
    "def all_subsections_from_section(\n",
    "    section: mwparserfromhell.wikicode.Wikicode,  # текущая секция\n",
    "    parent_titles: list[str],  # Заголовки родителя\n",
    "    sections_to_ignore: set[str],  # Секции, которые необходимо проигнорировать\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    Из раздела Википедии возвращает список всех вложенных секций.\n",
    "    Каждый подраздел представляет собой кортеж, где:\n",
    "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
    "      - второй элемент представляет собой текст секции\n",
    "    \"\"\"\n",
    "\n",
    "    # Извлекаем заголовки текущей секции\n",
    "    headings = [str(h) for h in section.filter_headings()]\n",
    "    title = headings[0]\n",
    "    # Заголовки Википедии имеют вид: \"== Heading ==\"\n",
    "\n",
    "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
    "        # Если заголовок секции в списке для игнора, то пропускаем его\n",
    "        return []\n",
    "\n",
    "    # Объединим заголовки и подзаголовки, чтобы сохранить контекст для chatGPT\n",
    "    titles = parent_titles + [title]\n",
    "\n",
    "    # Преобразуем wikicode секции в строку\n",
    "    full_text = str(section)\n",
    "\n",
    "    # Выделяем текст секции без заголовка\n",
    "    section_text = full_text.split(title)[1]\n",
    "    if len(headings) == 1:\n",
    "        # Если один заголовок, то формируем результирующий список\n",
    "        return [(titles, section_text)]\n",
    "    else:\n",
    "        first_subtitle = headings[1]\n",
    "        section_text = section_text.split(first_subtitle)[0]\n",
    "        # Формируем результирующий список из текста до первого подзаголовка\n",
    "        results = [(titles, section_text)]\n",
    "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
    "            results.extend(\n",
    "                # Вызываем функцию получения вложенных секций для заданной секции\n",
    "                all_subsections_from_section(subsection, titles, sections_to_ignore)\n",
    "            )  # Объединяем результирующие списки данной функции и вызываемой\n",
    "        return results\n",
    "\n",
    "\n",
    "# Функция возвращает список всех секций страницы, за исключением тех, которые отбрасываем\n",
    "def all_subsections_from_title(\n",
    "    title: str,  # Заголовок статьи Википедии, которую парсим\n",
    "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE,  # Секции, которые игнорируем\n",
    "    site_name: str = WIKI_SITE,  # Ссылка на сайт википедии\n",
    ") -> list[tuple[list[str], str]]:\n",
    "    \"\"\"\n",
    "    Из заголовка страницы Википедии возвращает список всех вложенных секций.\n",
    "    Каждый подраздел представляет собой кортеж, где:\n",
    "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
    "      - второй элемент представляет собой текст секции\n",
    "    \"\"\"\n",
    "\n",
    "    # Инициализация объекта MediaWiki\n",
    "    # WIKI_SITE ссылается на англоязычную часть Википедии\n",
    "    site = mwclient.Site(site_name)\n",
    "\n",
    "    # Запрашиваем страницу по заголовку\n",
    "    page = site.pages[title]\n",
    "\n",
    "    # Получаем текстовое представление страницы\n",
    "    text = page.text()\n",
    "\n",
    "    # Удобный парсер для MediaWiki\n",
    "    parsed_text = mwparserfromhell.parse(text)\n",
    "    # Извлекаем заголовки\n",
    "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
    "    if headings:  # Если заголовки найдены\n",
    "        # В качестве резюме берем текст до первого заголовка\n",
    "        summary_text = str(parsed_text).split(headings[0])[0]\n",
    "    else:\n",
    "        # Если нет заголовков, то весь текст считаем резюме\n",
    "        summary_text = str(parsed_text)\n",
    "    results = [([title], summary_text)]  # Добавляем резюме в результирующий список\n",
    "    for subsection in parsed_text.get_sections(\n",
    "        levels=[2]\n",
    "    ):  # Извлекаем секции 2-го уровня\n",
    "        results.extend(\n",
    "            # Вызываем функцию получения вложенных секций для заданной секции\n",
    "            all_subsections_from_section(subsection, [title], sections_to_ignore)\n",
    "        )  # Объединяем результирующие списки данной функции и вызываемой\n",
    "    return results\n",
    "\n",
    "\n",
    "# Разбивка статей на секции\n",
    "# придется немного подождать, так как на парсинг 100 статей требуется около минуты\n",
    "wikipedia_sections = []\n",
    "for title in titles:\n",
    "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
    "print(f\"Найдено {len(wikipedia_sections)} секций на {len(titles)} страницах\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a7e65a34",
   "metadata": {
    "title": "cell: clear"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отфильтровано 15 секций, осталось 249 секций.\n"
     ]
    }
   ],
   "source": [
    "# Очистка текста секции от ссылок <ref>xyz</ref>, начальных и конечных пробелов\n",
    "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
    "    titles, text = section\n",
    "    # Удаляем ссылки\n",
    "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
    "    # Удаляем пробелы вначале и конце\n",
    "    text = text.strip()\n",
    "    return (titles, text)\n",
    "\n",
    "\n",
    "# Применим функцию очистки ко всем секциям с помощью генератора списков\n",
    "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
    "\n",
    "\n",
    "# Отфильтруем короткие и пустые секции\n",
    "def keep_section(section: tuple[list[str], str]) -> bool:\n",
    "    \"\"\"Возвращает значение True, если раздел должен быть сохранен, в противном случае значение False.\"\"\"\n",
    "    titles, text = section\n",
    "    # Фильтруем по произвольной длине, можно выбрать и другое значение\n",
    "    if len(text) < 16:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "original_num_sections = len(wikipedia_sections)\n",
    "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
    "print(\n",
    "    f\"Отфильтровано {original_num_sections-len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)} секций.\"\n",
    ")\n",
    "# for ws in wikipedia_sections[:5]:\n",
    "#     print(ws[0])\n",
    "#     print(ws[1][:50] + \"...\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c63bddc",
   "metadata": {
    "title": "Cell:  test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249 секций Википедии поделены на 278 строк.\n"
     ]
    }
   ],
   "source": [
    "# Делим секции на части\n",
    "MAX_TOKENS = 1600\n",
    "wikipedia_strings = []\n",
    "for section in wikipedia_sections:\n",
    "    wikipedia_strings.extend(\n",
    "        split_strings_from_subsection(section, max_tokens=MAX_TOKENS)\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06f66e5d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Cell: tokening"
   },
   "outputs": [],
   "source": [
    "# Функция отправки chatGPT строки для ее токенизации (вычисления эмбедингов)\n",
    "def get_embedding(text, model=EMBEDDING_MODEL):\n",
    "    return OPENAI_CLIENT.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "def save_embeddings(path=SAVE_PATH):\n",
    "    df = pd.DataFrame({\"text\": wikipedia_strings[:10]})\n",
    "    df[\"embedding\"] = df.text.apply(lambda x: get_embedding(x, model=EMBEDDING_MODEL))\n",
    "    df.to_csv(path, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "GLOBAL_DATA = save_embeddings()\n",
    "\n",
    "\n",
    "def calculate_lines_csv():\n",
    "    df = GLOBAL_DATA\n",
    "    if df is None:\n",
    "        df = get_preset_data()\n",
    "    return len(df.text.str.split(\"\\n\").apply(len).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4104ed1",
   "metadata": {
    "title": "Cell: Telegram bot"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received SIGINT signal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@router.message(Command(\"start\", \"help\"))\n",
    "async def send_welcome(message: Message):\n",
    "    await message.reply(\n",
    "        f\"Hi! I'm ChatGPT-Winter-Olympics-Bot by @Ads_2s\\n\"\n",
    "        f\"I can answer questions about the {TOPIC}\\n\"\n",
    "        f\"I have {calculate_lines_csv()} lines in my database.\\n\"\n",
    "        \"---\\n\"\n",
    "        f\"Send me any message and I'll try to answer it.\\n\"\n",
    "        f\"Example:\\n\"\n",
    "        f\"What discipline was held at the 2022 Winter Olympics?\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "@router.message()\n",
    "async def gpt_answer(message: Message):\n",
    "    if message is not None:\n",
    "        await message.answer(ask(df=GLOBAL_DATA, query=\"What discipline was held at the 2022 Winter Olympics?\"))\n",
    "\n",
    "\n",
    "async def main():\n",
    "    dp.include_router(router)\n",
    "    await dp.start_polling(bot)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
