{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ads369/Ads_2s/blob/main/17_4_%D0%94%D0%BE%D0%BC%D0%B0%D1%88%D0%BD%D1%8F%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSPwDlw-MmJo"
   },
   "source": [
    "**Навигация по уроку**\n",
    "\n",
    "1. [Проблема переобучения нейронных сетей](https://colab.research.google.com/drive/10jxb_tGNTenMkxuC7ksTjjUtq966mZMM#scrollTo=SOSDEbSwa-Hj)\n",
    "2. [Методы оптимизации и регуляризации НС](https://colab.research.google.com/drive/1VWkA2xBTwWreo3DTdiMkio-RfQIvlXBl)\n",
    "3. [Универсальный алгоритм машинного обучения](https://colab.research.google.com/drive/1hK-5MC4eApd1-tRMppY9X9RLUYeMqG4q)\n",
    "4. Домашняя работа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNAPqodeKiw4"
   },
   "source": [
    "Используя знания данного урока, и набор данных IMDB вам необходимо:\n",
    "1. Спроектировать модель классификации отзывов к фильмам с точностью на валидационной выборке более 90%.\n",
    "2. Показать, что модель способна классифицировать отзывы с вероятностью более 88% на контрольной выборке.\n",
    "\n",
    "За успешное выполнение задания вы получите 3 балла. Если сможете преодолеть точность 95% на валидационной выборке и/или 93% на контрольной, то получите 4 балла.\n",
    "\n",
    "Также вы можете получить дополнительно 1 балл, если выполните все предложенные задания в задаче о Титанике (17.1), проанализируете \"увеличенную модель\" (17.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "pip"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425cbd4",
   "metadata": {
    "title": "Import"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    train_test_split,\n",
    ")\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721214f5",
   "metadata": {
    "title": "Main"
   },
   "outputs": [],
   "source": [
    "def time_execution(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        print(f\"Function {func.__name__} took {execution_time:.4f} seconds to execute.\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def resplit_dataset(train_data, test_data, train_labels, test_labels):\n",
    "    all_data = np.concatenate((train_data, test_data))\n",
    "    all_labels = np.concatenate((train_labels, test_labels))\n",
    "\n",
    "    # Resplit the data 80/20\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        all_data, all_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "def draw_baance_plot(data_tuple):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=data_tuple)\n",
    "    plt.title(\"Distribution of Labels in Training Data\")\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_balance(data_tuple):\n",
    "    unique, counts = np.unique(data_tuple, return_counts=True)\n",
    "    balance = dict(zip(unique, counts))\n",
    "    print(\"Balance of labels in training data:\")\n",
    "    print(f\"Totall: {len(data_tuple)}\")\n",
    "    print(f\"0 (Negative): {balance[0]}\")\n",
    "    print(f\"1 (Positive): {balance[1]}\")\n",
    "    print(f\"Ratio (Positive/Negative): {balance[1]/balance[0]:.2f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_and_preprocess_data():\n",
    "    (train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "        num_words=DICT_SPACE,\n",
    "        skip_top=20,\n",
    "    )\n",
    "\n",
    "    train_data, train_labels, test_data, test_labels = resplit_dataset(\n",
    "        train_data, test_data, train_labels, test_labels\n",
    "    )\n",
    "\n",
    "    calculate_balance(train_labels)\n",
    "\n",
    "    def vectorize_sequences(sequences, dimension=DICT_SPACE):\n",
    "        results = np.zeros((len(sequences), dimension))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            results[i, sequence] = 1.0\n",
    "        return results\n",
    "\n",
    "    x_train = vectorize_sequences(train_data)\n",
    "    x_test = vectorize_sequences(test_data)\n",
    "\n",
    "    y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "    y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "# Create model\n",
    "def create_model(layer_size=None, dropout_rate=None):\n",
    "    if layer_size is None:\n",
    "        layer_size = LAYER_SIZE\n",
    "\n",
    "    if dropout_rate is None:\n",
    "        dropout_rate = DROPOUT_RATE\n",
    "\n",
    "    model = models.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(DICT_SPACE,)),\n",
    "            layers.Dense(\n",
    "                layer_size, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)\n",
    "            ),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(\n",
    "                layer_size, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)\n",
    "            ),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            # layers.Dense(\n",
    "            #     layer_size, activation=\"relu\", kernel_regularizer=regularizers.l2(0.001)\n",
    "            # ),\n",
    "            # layers.Dropout(dropout_rate),\n",
    "            layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train model\n",
    "def train_model(model, x, y, x_val, y_val):\n",
    "    history = model.fit(\n",
    "        x,\n",
    "        y,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=0,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    raw_predictions = model.predict(x_test)\n",
    "    predictions = (raw_predictions > 0.5).astype(int)\n",
    "    score = accuracy_score(y_test, predictions)\n",
    "    return predictions, score\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Main execution\n",
    "@time_execution\n",
    "def main():\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    oos_y = []\n",
    "    oos_pred = []\n",
    "    fold_scores = []\n",
    "    the_best_score = 0.0\n",
    "    the_best_model = None\n",
    "    the_best_history = None\n",
    "    test_score = None\n",
    "\n",
    "    old_model = create_model()\n",
    "    old_history = None\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(x_train, y_train), 1):\n",
    "        print(f\"Training fold {fold}\")\n",
    "\n",
    "        _x_train, _x_val = x_train[train_index], x_train[val_index]\n",
    "        _y_train, _y_val = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        model = create_model()\n",
    "        history = train_model(model, _x_train, _y_train, _x_val, _y_val)\n",
    "\n",
    "        # Обучение общей модели\n",
    "        old_history = train_model(old_model, _x_train, _y_train, _x_val, _y_val)\n",
    "\n",
    "        if fold == 1:\n",
    "            print(model.summary())\n",
    "            plot_history(history)\n",
    "\n",
    "        # Predict and evaluate\n",
    "        val_pred, val_score = evaluate_model(model, _x_val, _y_val)\n",
    "\n",
    "        fold_scores.append(val_score)\n",
    "        print(f\"Fold {fold} validation accuracy: {val_score:.4f}\")\n",
    "\n",
    "        # Заполняем списки (реальными и предсказанными) данными, по которым не училась модель\n",
    "        oos_y.append(_y_val)\n",
    "        oos_pred.append(val_pred)\n",
    "\n",
    "        if the_best_score < val_score:\n",
    "            the_best_model = model\n",
    "            the_best_score = val_score\n",
    "            the_best_history = history\n",
    "\n",
    "    print(\"\\n---\")\n",
    "    print(f\"\\nMean accuracy: {np.mean(fold_scores):.4f}\")\n",
    "    print(f\"The best accuracy: {the_best_score:.4f}\")\n",
    "    print(\"\\n---\")\n",
    "\n",
    "    # CASE 1\n",
    "    # Выбрать модель с наивысшим score в качестве окончательной модели.\n",
    "    if the_best_model is not None and the_best_history is not None:\n",
    "        _, test_score = evaluate_model(the_best_model, x_test, y_test)\n",
    "        print(f\"\\nThe Best Model test accuracy: {test_score:.4f}\")\n",
    "        print(\n",
    "            f\"Final validation accuracy: {the_best_history.history['val_accuracy'][-1]:.4f}\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nThere is not the Best Model\")\n",
    "    print(\"---\")\n",
    "\n",
    "    # Case 2 - ?\n",
    "    # Предварительно установить новые данные для пяти моделей\n",
    "    # (по одному для каждого фолда) и усреднить результат.\n",
    "    # Вычисляем ошибку предсказания на всей накопленной в фолдах контрольной выборке.\n",
    "    oos_y = np.concatenate(oos_y)\n",
    "    oos_pred = np.concatenate(oos_pred)\n",
    "    score = accuracy_score(oos_y, oos_pred)\n",
    "    print(f\"\\nИтоговый score (accuracy): {score}\")\n",
    "    print(\"---\")\n",
    "\n",
    "    # Case 3\n",
    "    # Обучить новую модель на всём наборе данных, используя те же настройки (гиперпараметры),\n",
    "    # что и при перекрестной проверке: то же число эпох и та же структура слоёв.\n",
    "    final_model = create_model()\n",
    "    _history = train_model(final_model, x_train, y_train, x_test, y_test)\n",
    "    _, test_score = evaluate_model(final_model, x_test, y_test)\n",
    "    print(f\"\\nThe New Model accuracy: {test_score:.4f}\")\n",
    "    print(f\"Final validation accuracy: {_history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(\"---\")\n",
    "\n",
    "    # Case 4\n",
    "    # Проверка одной, модели которая училась на K-fold'aх\n",
    "    _, test_score = evaluate_model(the_best_model, x_test, y_test)\n",
    "    print(f\"\\nThe Old Model test accuracy: {test_score:.4f}\")\n",
    "    if old_history is not None:\n",
    "        print(\n",
    "            f\"Final validation accuracy: {old_history.history['val_accuracy'][-1]:.4f}\"\n",
    "        )\n",
    "    print(\"---\")\n",
    "\n",
    "    # Check goal achievements\n",
    "    val_accuracy = np.mean(fold_scores)\n",
    "    if val_accuracy > 0.90:\n",
    "        print(\"Task 1\")\n",
    "\n",
    "    if test_score:\n",
    "        if test_score > 0.88:\n",
    "            print(\"Task 2\")\n",
    "        if val_accuracy > 0.95 or test_score > 0.93:\n",
    "            print(\"Bonus task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f1d07",
   "metadata": {
    "title": "Cell grid search"
   },
   "outputs": [],
   "source": [
    "@time_execution\n",
    "def main_grid_search():\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "    # Создание KerasClassifier\n",
    "    model = KerasClassifier(\n",
    "        model=create_model, layer_size=LAYER_SIZE, dropout_rate=DROPOUT_RATE, verbose=0\n",
    "    )\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    # Определение диапазонов гиперпараметров для поиска\n",
    "    param_grid = {\n",
    "        # \"layer_size\": [4, 16, 32, 64, 128],\n",
    "        # 'optimizer': ['adam', 'rmsprop'],\n",
    "        \"layer_size\": [4, 16, 32],\n",
    "        \"dropout_rate\": [0.3, 0.5, 0.7],\n",
    "        \"batch_size\": [32, 64, 128, 256],\n",
    "        \"epochs\": [5, 10, 15],\n",
    "    }\n",
    "\n",
    "    if np.prod([len(v) for v in param_grid.values()]) > 10:\n",
    "        print(\"Using RandomizedSearchCV\")\n",
    "        grid = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_grid,\n",
    "            scoring=SCORING,\n",
    "            cv=kf,\n",
    "            n_iter=10,\n",
    "            random_state=42,\n",
    "            verbose=1,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using GridSearchCV\")\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring=SCORING,\n",
    "            cv=kf,\n",
    "            verbose=3,\n",
    "        )\n",
    "\n",
    "    # For debug\n",
    "    # print(\"Available parameters:\")\n",
    "    # print(model.get_params().keys())\n",
    "\n",
    "    # Запуск GridSearchCV\n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "    # Отображение лучших результатов\n",
    "    print(f\"Лучшие параметры: {grid_result.best_params_}\")\n",
    "    print(f\"Лучшая точность: {grid_result.best_score_:.4f}\")\n",
    "\n",
    "    # Оценка модели с лучшими параметрами на тестовой выборке\n",
    "    best_model = grid_result.best_estimator_\n",
    "    test_score = best_model.score(x_test, y_test)\n",
    "    print(f\"Точность на тестовой выборке: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5577fa1d",
   "metadata": {
    "title": "Constants"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "SCORING = \"accuracy\"\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "MAX_LEN = 400\n",
    "\n",
    "# It's  good work don't touch\n",
    "# EPOCHS = 10\n",
    "# BATCH_SIZE = 128\n",
    "# DICT_SPACE = 10000\n",
    "# NUM_FOLDS = 5\n",
    "# LAYER_SIZE = 16\n",
    "# DROPOUT_RATE = 0.5\n",
    "\n",
    "# Experemental zone\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "DICT_SPACE = 20000\n",
    "NUM_FOLDS = 5\n",
    "LAYER_SIZE = 32\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc592e8",
   "metadata": {
    "title": "Run"
   },
   "outputs": [],
   "source": [
    "main()\n",
    "# main_grid_search()\n",
    "# load_and_preprocess_data()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
